:markdown
  # Webhooks

  You can use webhooks to trigger actions in external services
  every time one of your scrapers finishes running.
  You can trigger all kind of fun actions:
  send emails when a scraper is finished;
  immediately pull fresh data from a scraper’s API;
  or run realtime analysis—anything you like.

  Add *webhook URLs* under the **Webhooks heading
  on your scraper’s Settings page**.
  Every time your scraper finishes running
  we’ll make an [HTTP POST request](https://en.wikipedia.org/wiki/POST_(HTTP))
  to each URL you’ve added.
  Use this request to trigger your custom code.

  ## Payloads

  We currently just make an empty HTTP POST request
  to your specified webhook URLs;
  no information about the scraper run
  is added as a payload.
  You can send information through the request
  by putting it in the webhook URL,
  either as the path or as GET parameters.
  The following UPPERCASE strings will be replaced with the current run details in the url:

  * ADDED: Records added count
  * AUTO: "true" if an automatic daily run, otherwise "false"
  * COMMIT: Full git revision
  * OUTCOME: "success" if run finished successfully, otherwise "failed"
  * REMOVED: Records removed count
  * REVISION: Abbreviated git revision (e.g. "c9fab2s7")
  * RUN_TIME: Seconds taken to run (eg "603" is 10 minutes and 3 seconds)
  * SCRAPER: Full scraper name, eg "owner-name/repo-name"
  * STATUS_CODE: Process exit code (0 means success)

  If there’s something specific you’d like included in the webhook payload
  then please [ask a question on our help forum](#{host_origin("help")}/).


